{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Emotion Recognition Training with FER-2013 Dataset\n",
        "This notebook trains a CNN model for facial emotion recognition using the FER-2013 dataset from Kaggle.\n",
        "\n",
        "**Dataset Source**: https://www.kaggle.com/datasets/msambare/fer2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the dataset structure\n",
        "data_dir = './dataset/emotion'\n",
        "print(\"Dataset structure:\")\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    level = root.replace(data_dir, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:5]:  # Show only first 5 files\n",
        "        print(f\"{subindent}{file}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"{subindent}... and {len(files) - 5} more files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define emotion classes (using all 7 emotions from FER-2013 dataset)\n",
        "emotion_classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "num_classes = len(emotion_classes)\n",
        "\n",
        "print(f\"Training with {num_classes} emotion classes: {emotion_labels}\")\n",
        "print(\"Using full FER-2013 dataset with all 7 emotion categories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation data generator (only rescaling)\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from directories (auto-detect all emotion classes)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=\"./dataset/emotion/train\",\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=\"./dataset/emotion/test\",\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Print detected classes and update our labels accordingly\n",
        "detected_classes = list(train_generator.class_indices.keys())\n",
        "print(f\"Detected emotion classes: {detected_classes}\")\n",
        "print(f\"Number of training samples: {train_generator.n}\")\n",
        "print(f\"Number of validation samples: {validation_generator.n}\")\n",
        "\n",
        "# Update num_classes based on actual detected classes\n",
        "num_classes = len(detected_classes)\n",
        "print(f\"Updated number of classes: {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CNN model for emotion recognition\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Flatten\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully-connected layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=RMSprop(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=3, \n",
        "    monitor='val_loss', \n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.n // validation_generator.batch_size,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "score = model.evaluate(validation_generator, verbose=0)\n",
        "print(f'Test Loss: {score[0]:.4f}')\n",
        "print(f'Test Accuracy: {score[1]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], 'r', label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save('Emotion1.h5')\n",
        "print(\"Model saved as 'Emotion1.h5'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "model_CNN = load_model('Emotion1.h5')\n",
        "\n",
        "# Define prediction labels (auto-detect from dataset)\n",
        "# Note: Order should match the training class indices\n",
        "try:\n",
        "    # Try to load class indices from a saved file or use default\n",
        "    predict_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "    print(f\"Using prediction labels: {predict_labels}\")\n",
        "except:\n",
        "    predict_labels = ['Emotion_0', 'Emotion_1', 'Emotion_2', 'Emotion_3', 'Emotion_4', 'Emotion_5', 'Emotion_6']\n",
        "    print(f\"Using default labels: {predict_labels}\")\n",
        "\n",
        "# Function to predict emotion from image path\n",
        "def predict_emotion(image_path):\n",
        "    try:\n",
        "        # Load and preprocess image\n",
        "        img = load_img(image_path, target_size=(150, 150))\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(img)\n",
        "        plt.title('Input Image')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "        # Convert to array and normalize\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = img_array.reshape(1, 150, 150, 3)\n",
        "        img_array = img_array.astype('float32') / 255.0\n",
        "        \n",
        "        # Make prediction\n",
        "        prediction = model_CNN.predict(img_array)\n",
        "        predicted_class = np.argmax(prediction, axis=-1)[0]\n",
        "        confidence = np.max(prediction)\n",
        "        \n",
        "        print(f\"Predicted Emotion: {predict_labels[predicted_class]}\")\n",
        "        print(f\"Confidence: {confidence:.2%}\")\n",
        "        \n",
        "        return predict_labels[predicted_class], confidence\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage (uncomment and provide a valid image path)\n",
        "# predict_emotion('./dataset/emotion/test/happy/sample_image.jpg')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
